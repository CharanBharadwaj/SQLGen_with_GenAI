import os
import torch
import random
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig, PeftModel
from langchain_core.prompts import PromptTemplate

random.seed(42)
torch.manual_seed(42)

model_path = "mistralai/Mistral-7B-Instruct-v0.1"
trained_snapshot = "mistral7Bv01-results/checkpoint-100"

lora_config = LoraConfig(
        r=16,
        lora_alpha=16,
        lora_dropout=0.05,
        bias='none',
        task_type='CAUSAL_LM',
        target_modules=[
              'q_proj',
              'k_proj',
              'v_proj',
              'o_proj'
            ]
    )

class LLMModel:
    def __init__(self,model_path=model_path,config=lora_config):
        # os.environ["PYTORCH_MPS_HIGH_WATERMARK_RATIO"] = "0.9"
        # os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, legacy=True, use_fast=True, add_eos_token=True, max_length=250)
        self.tokenizer.padding_side = 'right'
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        base_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype="auto"
        )
    
        self.model = PeftModel(base_model, lora_config)
        device = torch.device("mps")
        self.model.to(device)

    
    def create_prompt_for_test(self,sample):
        prompt_template = PromptTemplate.from_template("""Generate an SQL query using the following database schema:
{database_schema}, to answer the following question:
{user_question}. Also, suggest a chart type best suited to plot the data generated by the query.
Response:""")
        user_message = sample['question']
        #user_response = sample['answer']
        database_schema = sample['context']
    
        template = prompt_template.format(database_schema=database_schema,user_question=user_message)
        return template

    def generate_sql_query(self,natural_query,table_schema):
        test_query = {
            "question": natural_query,
            "context" : table_schema
        }
        
        
        model_inputs = self.tokenizer(self.create_prompt_for_test(test_query), return_tensors="pt").to("mps:0")
        
        output = self.model.generate(**model_inputs, max_length=500, 
                                     no_repeat_ngram_size=30, 
                                     pad_token_id=self.tokenizer.eos_token_id, 
                                     eos_token_id=self.tokenizer.eos_token_id
                                    )[0]
        
        result = self.tokenizer.decode(output, skip_special_tokens=False)
        return result
        